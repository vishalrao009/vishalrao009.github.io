<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" type="text/css" href="style.css">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css" />
    <title>GPU and CPU basics</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> 
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <style>
        body {
            background-image: url('../Images/parchment.jpg');
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
            margin: 0;
            font-family: Arial, sans-serif;
            padding: 3cm; /* Add 3cm padding on all sides */
        }

        #content {
            color: #333;
            font-size: 18px;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 10px;
            background-color: rgba(255, 255, 255, 0.5);
        }

        /* Mobile Styles */
        @media only screen and (max-width: 600px) {
            body {
                padding: 1cm; /* Adjust padding for smaller screens */
            }

            #content {
                padding: 10px; /* Adjust padding for smaller screens */
                max-width:100%; /*set maximum width for smaller devices */
            }
        }
    </style>
</head>
<body>
    <div id="content">
        

<div id="Contents" class="toc"><h1 id="Contents" class="header"><a href="#Contents">Contents</a></h1></div>
<ul>
<li>
<a href="GPU and CPU basics.html#Introduction:">#Introduction:</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Motivation">#Motivation</a>

<li>
<a href="GPU and CPU basics.html#GPU Architectures">#GPU Architectures</a>

<li>
<a href="GPU and CPU basics.html#Compute Capability:">#Compute Capability:</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Numbering Scheme:">#Numbering Scheme:</a>

<li>
<a href="GPU and CPU basics.html#Feature Support:">#Feature Support:</a>

<li>
<a href="GPU and CPU basics.html#CUDA Toolkit Compatibility:">#CUDA Toolkit Compatibility:</a>

<li>
<a href="GPU and CPU basics.html#Performance and Efficiency:">#Performance and Efficiency:</a>

<li>
<a href="GPU and CPU basics.html#Tensor Cores:">#Tensor Cores:</a>

</ul>
<li>
<a href="GPU and CPU basics.html#Pascal Architecture(chep@iisc cluster)">#Pascal Architecture(chep@iisc cluster)</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Number of cores (Pascal)">#Number of cores (Pascal)</a>

<li>
<a href="GPU and CPU basics.html#16nm FinFET Process Technology:">#16nm FinFET Process Technology:</a>

<li>
<a href="GPU and CPU basics.html#CUDA Cores and Streaming Multiprocessors (SMs):">#CUDA Cores and Streaming Multiprocessors (SMs):</a>

<li>
<a href="GPU and CPU basics.html#Simultaneous Multi-Projection (SMP):">#Simultaneous Multi-Projection (SMP):</a>

<li>
<a href="GPU and CPU basics.html#High Bandwidth Memory 2 (HBM2):">#High Bandwidth Memory 2 (HBM2):</a>

<li>
<a href="GPU and CPU basics.html#Unified Memory and Page Migration Engine:">#Unified Memory and Page Migration Engine:</a>

<li>
<a href="GPU and CPU basics.html#Performance Considerations:">#Performance Considerations:</a>

<li>
<a href="GPU and CPU basics.html#Enhanced Deep Learning Performance:">#Enhanced Deep Learning Performance:</a>

<li>
<a href="GPU and CPU basics.html#Pascal (mathematician / physicist)">#Pascal (mathematician / physicist)</a>

</ul>
<li>
<a href="GPU and CPU basics.html#CPU and GPU Differences">#CPU and GPU Differences</a>

<ul>
<li>
<a href="GPU and CPU basics.html#CPU Beats GPU">#CPU Beats GPU</a>

<li>
<a href="GPU and CPU basics.html#Current CPU Architecture">#Current CPU Architecture</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Introduction">#Introduction</a>

<li>
<a href="GPU and CPU basics.html#x86 Architecture:">#x86 Architecture:</a>

<li>
<a href="GPU and CPU basics.html#Compatibility:">#Compatibility:</a>

<li>
<a href="GPU and CPU basics.html#Memory Addressing:">#Memory Addressing:</a>

<li>
<a href="GPU and CPU basics.html#Instruction Set:">#Instruction Set:</a>

<li>
<a href="GPU and CPU basics.html#Widespread Adoption:">#Widespread Adoption:</a>

</ul>
<li>
<a href="GPU and CPU basics.html#64bit computing">#64bit computing</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Data Bus Width:">#Data Bus Width:</a>

<li>
<a href="GPU and CPU basics.html#Memory Addressing:">#Memory Addressing:</a>

<li>
<a href="GPU and CPU basics.html#Increased Memory Capacity:">#Increased Memory Capacity:</a>

<li>
<a href="GPU and CPU basics.html#Performance Improvements:">#Performance Improvements:</a>

<li>
<a href="GPU and CPU basics.html#Backward Compatibility:">#Backward Compatibility:</a>

<li>
<a href="GPU and CPU basics.html#Compatibility with 32-bit Systems:">#Compatibility with 32-bit Systems:</a>

<li>
<a href="GPU and CPU basics.html#Security Enhancements:">#Security Enhancements:</a>

</ul>
</ul>
</ul>
<li>
<a href="GPU and CPU basics.html#Properties of Our GPU (Tesla P100 )">#Properties of Our GPU (Tesla P100 )</a>

<ul>
<li>
<a href="GPU and CPU basics.html#Number of threads">#Number of threads</a>

<li>
<a href="GPU and CPU basics.html#Number of instructions per second">#Number of instructions per second</a>

</ul>
</ul>
<div id="Introduction:"><h1 id="Introduction:" class="header"><a href="#Introduction:">Introduction:</a></h1></div>

<p>
CUDA, which stands for Compute Unified Device Architecture, is a parallel computing platform and programming model developed by NVIDIA. It allows developers to use NVIDIA GPUs (Graphics Processing Units) for general-purpose parallel computing. CUDA enables efficient parallel processing for a wide range of applications beyond traditional graphics rendering, such as scientific simulations, deep learning, and numerical computations.<br />
</p>

<div id="Introduction:-Motivation"><h2 id="Motivation" class="header"><a href="#Introduction:-Motivation">Motivation</a></h2></div>

<p>
There are two reasons why we mostly use GPU over CPU:<br />
</p>
<ol>
<li>
Very High computational throughput
			computational throughput = Number of cores . clock speed . number of Instructions per cycle

<li>
High memeory bandwidth/throughput
			Memory bandwidth = (memory bus width in bits. Memory clock speed )

</ol>
<div id="Introduction:-GPU Architectures"><h2 id="GPU Architectures" class="header"><a href="#Introduction:-GPU Architectures">GPU Architectures</a></h2></div>

<p>
There are many kinds of Architecture available to use, each with varying compute capability. <br />
</p>


<table>
<thead>
<tr>
<th>
Architecture
</th>
<th>
Compute Capability
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Tesla
</td>
<td>
1.x
</td>
</tr>
<tr>
<td>
Fermi
</td>
<td>
2.x
</td>
</tr>
<tr>
<td>
Kepler
</td>
<td>
3.x
</td>
</tr>
<tr>
<td>
Maxwell
</td>
<td>
5.x
</td>
</tr>
<tr>
<td>
Pascal
</td>
<td>
6.x
</td>
</tr>
<tr>
<td>
Volta
</td>
<td>
7.x
</td>
</tr>
<tr>
<td>
Turing
</td>
<td>
7.5
</td>
</tr>
<tr>
<td>
Ampere
</td>
<td>
8.x
</td>
</tr>
</tbody>
</table>


<div id="Introduction:-Compute Capability:"><h2 id="Compute Capability:" class="header"><a href="#Introduction:-Compute Capability:">Compute Capability:</a></h2></div>

<p>
Compute Capability is a numerical value assigned to each NVIDIA GPU architecture, and it represents the level of features and capabilities supported by that architecture. It is an important parameter for developers, especially when writing CUDA (Compute Unified Device Architecture) code for GPU programming. Each compute capability introduces new features, instructions, and improvements over previous versions.<br />
</p>

<p>
Some key aspects related to Compute Capability include:<br />
</p>

<div id="Introduction:-Compute Capability:-Numbering Scheme:"><h3 id="Numbering Scheme:" class="header"><a href="#Introduction:-Compute Capability:-Numbering Scheme:">Numbering Scheme:</a></h3></div>
<p>
Compute Capability is represented in the form of major and minor version numbers (e.g., 6.1 for Pascal, 7.0 for Volta).<br />
</p>

<div id="Introduction:-Compute Capability:-Feature Support:"><h3 id="Feature Support:" class="header"><a href="#Introduction:-Compute Capability:-Feature Support:">Feature Support:</a></h3></div>
<p>
Higher compute capability generally implies support for newer features, instructions, and optimizations.<br />
</p>

<div id="Introduction:-Compute Capability:-CUDA Toolkit Compatibility:"><h3 id="CUDA Toolkit Compatibility:" class="header"><a href="#Introduction:-Compute Capability:-CUDA Toolkit Compatibility:">CUDA Toolkit Compatibility:</a></h3></div>
<p>
Some CUDA Toolkit versions may be designed to support specific compute capabilities. It's important for developers to ensure compatibility between their code and the targeted GPU architecture.<br />
</p>

<div id="Introduction:-Compute Capability:-Performance and Efficiency:"><h3 id="Performance and Efficiency:" class="header"><a href="#Introduction:-Compute Capability:-Performance and Efficiency:">Performance and Efficiency:</a></h3></div>
<p>
Newer compute capabilities often bring improvements in performance and energy efficiency, allowing developers to take advantage of advanced features and optimizations.<br />
</p>

<div id="Introduction:-Compute Capability:-Tensor Cores:"><h3 id="Tensor Cores:" class="header"><a href="#Introduction:-Compute Capability:-Tensor Cores:">Tensor Cores:</a></h3></div>
<p>
In more recent architectures like Volta, Turing, and Ampere, special hardware units called Tensor Cores were introduced to accelerate deep learning tasks.<br />
</p>


<div id="Introduction:-Pascal Architecture(chep@iisc cluster)"><h2 id="Pascal Architecture(chep@iisc cluster)" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)">Pascal Architecture(chep@iisc cluster)</a></h2></div>

<p>
We use 10 nodes of GPU. <br />
The NVIDIA Tesla P100 is a high-performance GPU designed for data center and high-performance computing (HPC) applications. The Tesla P100 PCIe variant has a compute capability of 6.0. The compute capability is a numerical representation that indicates the features and capabilities supported by the GPU architecture.<br />
</p>

<p>
For the Tesla P100 PCIe 16GB:<br />
</p>

<p>
Compute Capability: 6.0<br />
The compute capability is represented in the form of major and minor version numbers, where the major version indicates significant architectural changes and the minor version represents incremental improvements within that architecture.<br />
</p>

<p>
Here's a breakdown of the compute capability version 6.0:<br />
</p>

<p>
Major Version (6.x): Indicates the Pascal architecture.<br />
Minor Version (6.0): Specific incremental improvements and features within the Pascal architecture.<br />
Developers often use compute capability information when programming GPUs using frameworks like CUDA (Compute Unified Device Architecture). The compute capability helps ensure compatibility and optimization of code for specific GPU architectures.<br />
</p>


<p>
The Pascal architecture is a GPU (Graphics Processing Unit) architecture developed by NVIDIA and was introduced in 2016. It succeeded the Maxwell architecture and brought several improvements in terms of performance, energy efficiency, and new features. Pascal GPUs were designed for a variety of applications, including gaming, professional graphics, and high-performance computing.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Number of cores (Pascal)"><h3 id="Number of cores (Pascal)" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Number of cores (Pascal)">Number of cores (Pascal)</a></h3></div>

<p>
The NVIDIA Tesla P100 GPU features a number of streaming multiprocessors (SMs), and each SM contains a certain number of CUDA cores. To determine the number of CUDA cores on a Tesla P100 GPU, you can refer to the specifications provided by NVIDIA.<br />
</p>

<p>
Here are the key details for the Tesla P100 PCIe 16GB:<br />
</p>

<ul>
<li>
CUDA Cores per SM (Streaming Multiprocessor): 64 CUDA cores

<li>
Number of SMs: 56 SMs

</ul>
<p>
Now, you can calculate the total number of CUDA cores by multiplying the number of CUDA cores per SM by the number of SMs:<br />
</p>

<p>
Total CUDA Cores = CUDA Cores per SM x Number of SMs <br />
</p>

<p>
For the Tesla P100:<br />
</p>

<p>
Total CUDA Cores = 64 x 56 = 3584  CUDA Cores<br />
</p>

<p>
So, the Tesla P100 PCIe 16GB has a total of 3,584 CUDA cores. Keep in mind that this information is specific to the Tesla P100 GPU architecture, and different GPU models may have different configurations. Always refer to the official specifications provided by the GPU manufacturer for accurate and up-to-date information.<br />
</p>

<p>
Use <code>nvidia-smi -h</code> for details. <br />
</p>

<p>
Here are key features and characteristics of the Pascal architecture:<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-16nm FinFET Process Technology:"><h3 id="16nm FinFET Process Technology:" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-16nm FinFET Process Technology:">16nm FinFET Process Technology:</a></h3></div>
<p>
Pascal GPUs were manufactured using a 16nm FinFET process technology, a significant advancement over the 28nm process used in the Maxwell architecture. This transition to a smaller process node contributed to improved energy efficiency and performance.<br />
</p>

<p>
<a href="Pascal_FinFET Technology.html">Pascal_FinFET Technology</a><br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-CUDA Cores and Streaming Multiprocessors (SMs):"><h3 id="CUDA Cores and Streaming Multiprocessors (SMs):" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-CUDA Cores and Streaming Multiprocessors (SMs):">CUDA Cores and Streaming Multiprocessors (SMs):</a></h3></div>
<p>
Pascal GPUs featured an increase in the number of CUDA cores compared to Maxwell. CUDA cores are the processing units responsible for executing parallel tasks. The architecture maintained the Streaming Multiprocessor (SM) design, with each SM containing multiple CUDA cores. (For chep@iisc, we have Pascal 6.0, which have 56 SM per node,  where each SM has 64 cores).<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Simultaneous Multi-Projection (SMP):"><h3 id="Simultaneous Multi-Projection (SMP):" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Simultaneous Multi-Projection (SMP):">Simultaneous Multi-Projection (SMP):</a></h3></div>
<p>
Pascal introduced Simultaneous Multi-Projection, a feature designed to enhance virtual reality (VR) and multi-display experiences. SMP enables more efficient rendering for multiple projections, improving the visual quality and realism in VR applications.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-High Bandwidth Memory 2 (HBM2):"><h3 id="High Bandwidth Memory 2 (HBM2):" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-High Bandwidth Memory 2 (HBM2):">High Bandwidth Memory 2 (HBM2):</a></h3></div>
<p>
Some Pascal GPUs, particularly those in the professional and data center-oriented Tesla and Quadro series, adopted High Bandwidth Memory 2 (HBM2). HBM2 provides higher memory bandwidth compared to traditional GDDR5 memory, which is beneficial for memory-intensive applications.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Unified Memory and Page Migration Engine:"><h3 id="Unified Memory and Page Migration Engine:" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Unified Memory and Page Migration Engine:">Unified Memory and Page Migration Engine:</a></h3></div>
<p>
Pascal continued to support Unified Memory, allowing developers to access both CPU and GPU memory seamlessly. The introduction of the Page Migration Engine enhanced the efficiency of data transfers between CPU and GPU memory.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Performance Considerations:"><h3 id="Performance Considerations:" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Performance Considerations:">Performance Considerations:</a></h3></div>
<p>
Pascal GPUs demonstrated improvements in performance across various applications, including gaming, content creation, and parallel computing. The architecture was designed to provide a balance of computational power, memory bandwidth, and energy efficiency.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Enhanced Deep Learning Performance:"><h3 id="Enhanced Deep Learning Performance:" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Enhanced Deep Learning Performance:">Enhanced Deep Learning Performance:</a></h3></div>

<p>
Pascal GPUs showed notable improvements in deep learning performance. While not as specialized as later architectures like Volta and Turing, Pascal GPUs were still capable of handling deep learning workloads efficiently.<br />
NVLink:<br />
</p>

<p>
Pascal introduced NVLink, a high-speed interconnect technology primarily used in data center and high-performance computing (HPC) environments. NVLink enables faster communication between multiple GPUs, supporting scalable and efficient parallel processing.<br />
It's important to note that the Pascal architecture was succeeded by the Volta architecture and later by Turing and Ampere. Each subsequent architecture introduced new features and optimizations, especially in the context of AI and deep learning, but Pascal GPUs remain relevant for various applications.<br />
</p>

<div id="Introduction:-Pascal Architecture(chep@iisc cluster)-Pascal (mathematician / physicist)"><h3 id="Pascal (mathematician / physicist)" class="header"><a href="#Introduction:-Pascal Architecture(chep@iisc cluster)-Pascal (mathematician / physicist)">Pascal (mathematician / physicist)</a></h3></div>

<p>
Pascal is unit of pressure, 1 atm = 10<sup><small>5 pascal, where one pascal is 1 N/m</small></sup>2<br />
</p>

<div id="Introduction:-CPU and GPU Differences"><h2 id="CPU and GPU Differences" class="header"><a href="#Introduction:-CPU and GPU Differences">CPU and GPU Differences</a></h2></div>

<table>
<thead>
<tr>
<th>
Feature
</th>
<th>
CPU
</th>
<th>
GPU
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Function
</td>
<td>
General Purpose processing
</td>
<td>
Specialized for parallel tasks
</td>
</tr>
<tr>
<td>
Architecture
</td>
<td>
few cores
</td>
<td>
Many cores
</td>
</tr>
<tr>
<td>
Parallelism
</td>
<td>
single threaded
</td>
<td>
can use so many threads
</td>
</tr>
<tr>
<td>
Memory Hierarchy
</td>
<td>
complex, low latency throughput
</td>
<td>
High throughput parallel access
</td>
</tr>
<tr>
<td>
Flexibility
</td>
<td>
Versatile, Handles various workloads
</td>
<td>
specialized for specific tasks
</td>
</tr>
<tr>
<td>
Execution type
</td>
<td>
Sequential
</td>
<td>
parallel
</td>
</tr>
</tbody>
</table>


<p>
Please note that this table provides a high-level overview, and the specifics can vary based on the architecture and design of individual CPUs and GPUs. Additionally, advancements in technology may lead to changes in the characteristics of future CPU and GPU architectures.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-CPU Beats GPU"><h3 id="CPU Beats GPU" class="header"><a href="#Introduction:-CPU and GPU Differences-CPU Beats GPU">CPU Beats GPU</a></h3></div>

<table>
<thead>
<tr>
<th>
Task
</th>
<th>
CPU Advantage
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Single-Threaded Work
</td>
<td>
Higher clock speeds for individual tasks.
</td>
</tr>
<tr>
<td>
Sequential Algorithms
</td>
<td>
Better for step-by-step execution.
</td>
</tr>
<tr>
<td>
Task Switching
</td>
<td>
Efficient handling of diverse concurrent tasks.
</td>
</tr>
<tr>
<td>
General-Purpose Computing
</td>
<td>
Versatility for various computing tasks.
</td>
</tr>
<tr>
<td>
Operating System Operations
</td>
<td>
Integral role in managing system operations.
</td>
</tr>
<tr>
<td>
Data Serialization
</td>
<td>
Sequential processing for dependent data.
</td>
</tr>
<tr>
<td>
Low-Latency Access
</td>
<td>
Complex memory hierarchy for quick data retrieval.
</td>
</tr>
</tbody>
</table>

<p>
It's important to note that the superiority of CPUs in these scenarios does not imply that GPUs cannot perform these tasks. Rather, CPUs are typically more optimized for these specific types of workloads. The choice between CPU and GPU often depends on the nature of the task and the specific requirements of the application.<br />
</p>


<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture"><h3 id="Current CPU Architecture" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture">Current CPU Architecture</a></h3></div>

<p>
I am using a rocky linux 9.2, (podman container) , which by default makes me root user(su/ super user). The OS is x86_64 compatible, means that the CPU cores (20 cores of chep cluster) is x86_64 processor.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-Introduction"><h4 id="Introduction" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-Introduction">Introduction</a></h4></div>
<p>
The term "x86_64" refers to a particular computer architecture that is widely used in modern CPUs. It is an extension of the original x86 architecture. Here's a breakdown of what "x86_64" means:<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-x86 Architecture:"><h4 id="x86 Architecture:" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-x86 Architecture:">x86 Architecture:</a></h4></div>
<p>
The term "x86" originally referred to a family of instruction set architectures for certain processors, particularly those developed by Intel. The name "x86" is derived from the 86 in "8086," which was one of the early processors in this family.<br />
64-bit Extension:<br />
</p>

<p>
The "64" in "x86_64" indicates that this architecture is an extension to the original x86 architecture to support 64-bit computing. In the context of CPUs, a 64-bit architecture allows the processor to handle larger amounts of memory and perform computations involving 64-bit integers.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-Compatibility:"><h4 id="Compatibility:" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-Compatibility:">Compatibility:</a></h4></div>
<p>
x86_64 CPUs are designed to be backward compatible with the earlier 32-bit x86 architecture. This means they can run software that was written for 32-bit x86 processors. However, they also introduce additional features and capabilities associated with 64-bit computing.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-Memory Addressing:"><h4 id="Memory Addressing:" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-Memory Addressing:">Memory Addressing:</a></h4></div>
<p>
One of the significant advantages of the x86_64 architecture is its ability to address larger amounts of memory. While 32-bit architectures are limited to addressing 4 GB of RAM directly, x86_64 allows for a much larger address space, theoretically supporting up to 18.4 million TB (terabytes) of RAM.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-Instruction Set:"><h4 id="Instruction Set:" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-Instruction Set:">Instruction Set:</a></h4></div>
<p>
The x86_64 architecture retains the x86 instruction set but introduces new 64-bit instructions. This allows the CPU to handle larger data sizes and perform more complex computations.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-Current CPU Architecture-Widespread Adoption:"><h4 id="Widespread Adoption:" class="header"><a href="#Introduction:-CPU and GPU Differences-Current CPU Architecture-Widespread Adoption:">Widespread Adoption:</a></h4></div>
<p>
x86_64 has become the dominant architecture for personal computers and servers. Most modern desktop and laptop CPUs, as well as many server CPUs, use the x86_64 architecture.<br />
Commonly Referred to as AMD64:<br />
</p>

<p>
While initially introduced by AMD (Advanced Micro Devices), the x86_64 architecture is now commonly referred to as AMD64. Intel later adopted the same architecture and used the term "Intel 64" to refer to their implementation.<br />
In summary, x86_64 is an extension of the x86 architecture that brings 64-bit computing capabilities to CPUs. It has become the standard architecture for most desktop, laptop, and server processors due to its backward compatibility and support for larger memory addressing.<br />
</p>


<div id="Introduction:-CPU and GPU Differences-64bit computing"><h3 id="64bit computing" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing">64bit computing</a></h3></div>
<p>
64-bit computing refers to the use of processors that have a data bus, address bus, or both that are 64 bits wide. This specification refers to the amount of data that a processor can handle in a single instruction or the size of memory addresses. Here are the key aspects of 64-bit computing:<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Data Bus Width:"><h4 id="Data Bus Width:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Data Bus Width:">Data Bus Width:</a></h4></div>
<p>
In a 64-bit processor, the data bus is 64 bits wide. This means that the processor can handle 64 bits of data in a single instruction. The wider data bus allows for more data to be processed simultaneously, potentially leading to improved performance for certain types of computations.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Memory Addressing:"><h4 id="Memory Addressing:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Memory Addressing:">Memory Addressing:</a></h4></div>
<p>
One of the significant advantages of 64-bit computing is the ability to address larger amounts of memory. With a 64-bit address bus, a processor can theoretically address up to 2^64 individual memory locations. This translates to an enormous address space, allowing systems to support much larger amounts of RAM compared to 32-bit systems.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Increased Memory Capacity:"><h4 id="Increased Memory Capacity:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Increased Memory Capacity:">Increased Memory Capacity:</a></h4></div>
<p>
64-bit processors enable systems to access and use more RAM. While 32-bit systems are limited to addressing 4 GB of RAM directly, 64-bit systems can address several terabytes of RAM. This is particularly beneficial for memory-intensive applications and tasks.<br />
The maximum amount of RAM that an x86_64 bit (64-bit) Intel processor can manage depends on various factors, including the specific processor architecture, the motherboard, and the operating system. However, in theory, the x86_64 architecture supports a vast amount of addressable memory.<br />
</p>


<div id="Introduction:-CPU and GPU Differences-64bit computing-Performance Improvements:"><h4 id="Performance Improvements:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Performance Improvements:">Performance Improvements:</a></h4></div>
<p>
Certain types of applications, especially those dealing with large datasets or complex calculations, may experience performance improvements when running on a 64-bit system. The wider data bus allows for more efficient processing of larger chunks of data in each instruction.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Backward Compatibility:"><h4 id="Backward Compatibility:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Backward Compatibility:">Backward Compatibility:</a></h4></div>
<p>
64-bit processors are designed to be backward compatible with 32-bit software. This means that they can run both 32-bit and 64-bit applications. However, to fully utilize the benefits of 64-bit computing, applications need to be compiled as 64-bit versions.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Compatibility with 32-bit Systems:"><h4 id="Compatibility with 32-bit Systems:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Compatibility with 32-bit Systems:">Compatibility with 32-bit Systems:</a></h4></div>
<p>
While 64-bit processors can run 32-bit software, the reverse is not true. A 32-bit processor cannot directly run 64-bit software. Therefore, systems with 64-bit processors can accommodate both 32-bit and 64-bit applications.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Security Enhancements:"><h4 id="Security Enhancements:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Security Enhancements:">Security Enhancements:</a></h4></div>
<p>
64-bit computing allows for certain security enhancements, including the ability to implement more advanced security features at the hardware level. This can contribute to improved system security.<br />
In summary, 64-bit computing provides advantages in terms of larger memory addressing, increased RAM capacity, and potential performance improvements for certain types of applications. It has become the standard for modern desktops, laptops, and servers, allowing for more powerful and capable computing systems.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:"><h4 id="Here are the theoretical limits for the x86_64 architecture:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:">Here are the theoretical limits for the x86_64 architecture:</a></h4></div>

<p>
Theoretical Maximum Addressable Memory:<br />
</p>

<p>
An x86_64 processor can theoretically address 2^(64)  memory locations.<br />
This translates to 18.4 million terabytes (TB) of addressable memory.<br />
</p>
<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Practical Limits:"><h5 id="Practical Limits:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Practical Limits:">Practical Limits:</a></h5></div>
<p>
While the theoretical limit is extremely high, the practical limit is determined by other factors, such as the limitations of current memory technology, motherboard design, and the operating system.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Current Real-World Limits:"><h5 id="Current Real-World Limits:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Current Real-World Limits:">Current Real-World Limits:</a></h5></div>
<p>
As of my last knowledge update in January 2022, common x86_64 desktop and server systems support up to several terabytes of RAM.<br />
High-end servers and workstations may support memory configurations ranging from 256 GB to multiple terabytes.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Operating System and Hardware Considerations:"><h5 id="Operating System and Hardware Considerations:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Operating System and Hardware Considerations:">Operating System and Hardware Considerations:</a></h5></div>
<p>
The maximum amount of supported RAM can also depend on the operating system. For example, Windows and Linux have different limitations.<br />
The motherboard architecture, chipset, and BIOS/UEFI firmware play crucial roles in determining the maximum supported memory.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Intel Specifics:"><h5 id="Intel Specifics:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Intel Specifics:">Intel Specifics:</a></h5></div>
<p>
Different Intel processors may have varying levels of support for memory. Some high-end Intel processors designed for servers or workstations may support larger amounts of RAM.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-ECC (Error-Correcting Code) Memory:"><h5 id="ECC (Error-Correcting Code) Memory:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-ECC (Error-Correcting Code) Memory:">ECC (Error-Correcting Code) Memory:</a></h5></div>
<p>
ECC memory support can also affect the maximum amount of RAM a system can handle. ECC memory is commonly used in server environments for enhanced reliability but may have specific limitations.<br />
</p>

<div id="Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Future Developments:"><h5 id="Future Developments:" class="header"><a href="#Introduction:-CPU and GPU Differences-64bit computing-Here are the theoretical limits for the x86_64 architecture:-Future Developments:">Future Developments:</a></h5></div>
<p>
As technology evolves, new processor architectures and memory technologies may increase the practical limits of addressable memory.<br />
</p>




<div id="Properties of Our GPU (Tesla P100 )"><h1 id="Properties of Our GPU (Tesla P100 )" class="header"><a href="#Properties of Our GPU (Tesla P100 )">Properties of Our GPU (Tesla P100 )</a></h1></div>

<table>
<thead>
<tr>
<th>
Property
</th>
<th>
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
GPU Architecture
</td>
<td>
Pascal
</td>
</tr>
<tr>
<td>
CUDA Cores
</td>
<td>
3,584
</td>
</tr>
<tr>
<td>
Memory Size
</td>
<td>
16 GB HBM2
</td>
</tr>
<tr>
<td>
Memory Interface
</td>
<td>
4096-bit
</td>
</tr>
<tr>
<td>
Memory Bandwidth
</td>
<td>
732 GB/s
</td>
</tr>
<tr>
<td>
Base Clock
</td>
<td>
Approximately 1328 MHz
</td>
</tr>
<tr>
<td>
Boost Clock
</td>
<td>
Varies(boosts higher under certain conditions)
</td>
</tr>
<tr>
<td>
Compute Capability
</td>
<td>
6.0
</td>
</tr>
<tr>
<td>
Streaming Multiprocessors (SMs)
</td>
<td>
56
</td>
</tr>
<tr>
<td>
Maximum Concurrent Warps per SM
</td>
<td>
64 (64 core or one warp(32 threads) per core)
</td>
</tr>
<tr>
<td>
Total Threads per GPU
</td>
<td>
114,688
</td>
</tr>
<tr>
<td>
GPU Direct (RDMA) Support
</td>
<td>
Yes (for certain configurations)
</td>
</tr>
<tr>
<td>
Memory ECC (Error-Correcting Code)
</td>
<td>
Yes
</td>
</tr>
<tr>
<td>
L2 Cache Size per SM
</td>
<td>
4 MB
</td>
</tr>
<tr>
<td>
Transistors
</td>
<td>
Approximately 15.3 billion
</td>
</tr>
<tr>
<td>
Manufacturing Process
</td>
<td>
16 nm
</td>
</tr>
<tr>
<td>
Form Factor
</td>
<td>
PCIe (available in various form factors)
</td>
</tr>
<tr>
<td>
TDP (Thermal Design Power)
</td>
<td>
Typically around 300W
</td>
</tr>
<tr>
<td>
NVIDIA GPU Boost
</td>
<td>
Yes
</td>
</tr>
<tr>
<td>
GPU Virtualization (vGPU)
</td>
<td>
Yes (with NVIDIA GRID)
</td>
</tr>
<tr>
<td>
NVIDIA NVLink
</td>
<td>
Yes (for certain configurations)
</td>
</tr>
<tr>
<td>
NVIDIA GPUDirect RDMA
</td>
<td>
Yes
</td>
</tr>
</tbody>
</table>

<p>
Please note that the total number of threads per GPU is based on the CUDA cores and the number of threads per warp.<br />
</p>

<div id="Properties of Our GPU (Tesla P100 )-Number of threads"><h2 id="Number of threads" class="header"><a href="#Properties of Our GPU (Tesla P100 )-Number of threads">Number of threads</a></h2></div>

<p>
For Tesla P100 which is Pascal based architecture with compute capability 6.0 has 56 SM. Each SM has 64 cores. Each core can run one warp (32 threads parallelly) parallelly at a time. So for a given GPU node we have 56 x 64 x 32 = 114688  parallel threads per node. <br />
</p>

<p>
Parallel threads per core = 32 = (1 warp)<br />
Parallel threads per SM = 64 x 32 = 2048 (32 warps)<br />
Parallel threads per node = 56 x Parallel threads per SM = 114688 instructions per clock cycle.<br />
</p>

<p>
The NVIDIA Tesla P100 has a base clock of 1190 MHz and a boost clock of 1329 MHz. It also has a memory clock of 715 MHz/1430 Mbps effective. <br />
The Tesla P100 is a dual-slot card that draws power from a single 8-pin power connector. It has a maximum power draw of 250 W. <br />
The Tesla P100 was released on April 5, 2016. It has a Pascal architecture and 3584 CUDA cores<br />
</p>

<div id="Properties of Our GPU (Tesla P100 )-Number of instructions per second"><h2 id="Number of instructions per second" class="header"><a href="#Properties of Our GPU (Tesla P100 )-Number of instructions per second">Number of instructions per second</a></h2></div>

<p>
Given the clock cycle of 1329 MHz, in one second it will execute : 1329 x 114688 (Total number of threads per node) = 152420352000000 instructions per second(IPS). (floating point operations per second(FLOPS)=Instructions per second/ Number of instructions for Floating point operations) or 1.52420352e13 FLOPS.(If we require 10 instructions for one FLOP)<br />
</p>

<p>
FLOPS: Floating Point Operations Per Second<br />
NFLO: Number of Floating Point Operations<br />
IPS : Instructions Per Second<br />
</p>

<div id="Properties of Our GPU (Tesla P100 )-References"><h2 id="References" class="header"><a href="#Properties of Our GPU (Tesla P100 )-References">References</a></h2></div>
<ol>
<li>
<a href="https://www.nvidia.com/en-in/">https://www.nvidia.com/en-in/</a>

<li>
<a href="https://chat.openai.com">https://chat.openai.com</a>

</ol>

    </div>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
    <script type="text/javascript">
        document.querySelectorAll('pre').forEach(block => hljs.highlightBlock(block));
    </script>
</body>
</html>

