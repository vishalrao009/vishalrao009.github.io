<!DOCTYPE html>
<html>
<head>
    <link rel="Stylesheet" type="text/css" href="../style.css">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/default.min.css" />
    <title>try13_program_functions</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            background-image: url('../Images/parchment.jpg');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            opacity: 0.0; /* Adjust the opacity value */
            color: #fff; /* Set text color */
        }

        a {
            color: #001f3f; /* Set link color to dark blue */
            text-decoration: none; /* Remove underline if needed */
        }

        header, nav, main, footer {
            display: none; /* Hide header, nav, main, and footer */
        }
    </style>
</head>
<body>
    <main>
        

<p>
This sections contains the functions: 
</p>
<pre python>

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def filtering(input_file_address,destination_file_address,keyword):
    """
    This function takes input file, and stores each line which contains the keyword
    """
    with open(input_file_address, "r") as readfile1:
        content=readfile1.readlines()
        with open(destination_file_address,"w") as writefile:
            for i in range(0, len(content)):
                if (content[i].find(keyword)!=-1):
                    writefile.write(content[i])
def filtering_list(input_list,keyword):
    """
    This function takes input list, and stores each line which contains the keyword
    """
    out_list=[]
    for element in input_list:
        if (element.find(keyword)!=-1):
            out_list.append(element)
    return out_list


def advanced_filtering(input_file,output_file,keyword):
    """
    this function just writes the value of quantity with name keyword in output file
    """
    with open(input_file,"r") as readfile:
        line_list=readfile.readlines()
        with open(output_file,"w") as writefile:
            for i in range(0, len(line_list)):
                if (line_list[i].find(keyword)!=-1):
                    writefile.write(line_list[i][line_list[i].find(keyword)+len(keyword):])

def advanced_filtering_list(input_list,keyword):
    """
    this function just writes the value of quantity with name keyword in output file
    """
    output_list=[]
    for i in range(0, len(input_list)):
        start_index=str(input_list[i]).find(keyword)
        if (start_index!=-1):
            # print ((input_list[i][start_index+len(keyword):]))
            output_list.append(float(input_list[i][start_index+len(keyword):]))
    return output_list

def accepted_filtering(inputfile,outputfile,keyword):
    """
    This function will be able to generate the measurements which are accepted in output file,
    here keyword we will be using while calling the function is : Update accepted
    For further use, the output file shall be used for advanced filtering
    , Since there are 28 rows of data for each measurement, we will have number 28 use in loop somewhere.
    """
    with open(inputfile,"r") as readfile:
        line_list=readfile.readlines()
        with open(outputfile,"w") as writefile:
            for i in range(0,len(line_list)):
                index=line_list[i].find(keyword)
                if (index!=-1):
                    for j in range(0,28):
                        writefile.write(line_list[i+j])
                    i+=28

def accepted_filtering_list(input_list,keyword):
    """
    This function will be able to generate the measurements which are accepted in output file,
    here keyword we will be using while calling the function is : Update accepted
    For further use, the output file shall be used for advanced filtering
    , Since there are 28 rows of data for each measurement, we will have number 28 use in loop somewhere.
    """
    output_list=[]
    for i in range(0,len(input_list)):
        index=input_list[i].find(keyword)
        if (index!=-1):
            for j in range(0,29):
                output_list.append(input_list[(i-1)+j])
    return output_list

def chiral_condensate_mean(input_file,output_file):
    """
    This function calculates the mean of 10 numbers which are generated from advanced_filtering function
    """
    with open(input_file, "r") as readfile:
        content_list=readfile.readlines();
        index=0
        with open(output_file,"w") as writefile:
            for i in range(0,int(len(content_list)/10)):
                total=0
                for j in range(index,index+10):
                    total+=float(content_list[j])
                index+=10
                writefile.write(str(round(total/10,9)))
                writefile.write("\n")

def chiral_condensate_mean_list(input_list):
    """
    This function calculates the mean of 10 numbers which are generated from advanced_filtering function.
    """
    index=0
    output_list=[]
    for i in range(0,int(len(input_list)/10)):
        total=0
        for j in range(index,index+10):
            total+=float(input_list[j])
        index+=10
        output_list.append(round(total/10,9))
    return output_list

def acceptance_ratio(inputfile):
    """This function calcuates and returns the acceptance ratio"""
    # print("from acceptance_ratio",inputfile)
    n_accepted=0
    n_declined=0
    with open(inputfile,"r") as readfile:
        content_list=readfile.readlines()
    i=0
    while(i&lt;len(content_list)-3):
        element=content_list[i]
        if (element.find("Update accepted!")!=-1):
            n_accepted+=1
            i+=3
        elif (element.find("Update declined!")!=-1):
            n_declined+=1
            i+=3
        else:
            i+=1
    ratio=n_accepted/(n_declined+n_accepted)
    # print(ratio)
    return ratio

def acceptance_ratio_list(input_list):
    """This function calcuates and returns the acceptance ratio"""
    # print("from acceptance_ratio",inputfile)
    n_accepted=0
    n_declined=0
    i=0
    while(i&lt;len(input_list)-3):
        element=input_list[i]
        if (element.find("Update accepted!")!=-1):
            n_accepted+=1
            i+=3
        elif (element.find("Update declined!")!=-1):
            n_declined+=1
            i+=3
        else:
            i+=1
    ratio=n_accepted/(n_declined+n_accepted)
    # print(ratio)
    return ratio



def clean_list_float(list_item):
    """This function turns string list to float"""
    desired_list=[]
    for element in list_item:
        desired_list.append(float(element))
    return desired_list

def clean_list_int(list_item):
    """This function turns string elements of list to integer"""
    desired_list=[]
    for element in list_item:
        desired_list.append(int(element))
    return desired_list


def plot_fun(file1,file2,color):
    """Plots fil1 vs file2 on x and y axis respectively and store them on destination_directory"""
    with open(file1,"r") as readfile1:
        with open(file2,"r") as readfile2:
            file1_list=clean_list_int(readfile1.readlines())
            file2_list=clean_list_float(readfile2.readlines())
    plt.figure()
    plt.plot(file1_list,file2_list,color,label=file2)
    plt.legend()
    plt.xlabel("Measurement index / Time")
    plt.ylabel(file2)
    path=file2+".png"
    plt.savefig(path) #destination_dir shall contain '/' in end
    plt.clf()
    plt.close()
    # plt.show()

def plot_fun_list(list1,list2,color,path_to_save,list_2_label_name,lim_list=[],correlation_time="none"):
    """Plots list1 vs list2 on x and y axis respectively and store them on destination_directory"""
    plt.figure()
    if correlation_time=="none":
        plt.plot(list1,list2,color,label=path_to_save)
    elif correlation_time!="none":
        plt.plot(list1,list2,color,label=f"correlation time: {correlation_time}")
    if len(lim_list)!=0:           # we will set limits only if provided
        plt.xlim(lim_list[0],lim_list[1])
    plt.legend()
    plt.xlabel("Measurement index / Time")
    plt.ylabel(list_2_label_name)
    plt.savefig(path_to_save) #destination_dir shall contain '/' in end
    plt.clf()
    plt.close()

def mean_list(input_list):
    """ this function is expected to return the mean, standard deviation about mean of input list """
    total=0
    for element in input_list:
        total+=float(element)
    mean=total/len(input_list)
    return mean
def mean_file(input_file):
    """ this function is expected to return the mean, standard deviation about mean of input file containing only numbers in a column"""
    with open(input_file,"r") as readfile:
        file_content_list=readfile.readlines()
        return mean_list(file_content_list)
def standard_deviation_list(input_list):
    """Calculates the standard deviation given input file"""
    total=0
    mean=mean_list(input_list)
    for element in input_list:
        total+=(float(element)-mean)**2
    var=total/len(input_list)
    std_deviation=var**0.5
    return std_deviation
def standard_deviation_file(input_file):
    """Calculates the standard deviation given input file"""
    total=0
    with open(input_file,"r") as readfile:
        file_content_list=readfile.readlines()
    return standard_deviation_list(file_content_list)



import os
def get_file_names(directory_path):
    # Check if the provided path is a directory
    if not os.path.isdir(directory_path):
        print(f"{directory_path} is not a valid directory.")
        return
    # Get a list of all files in the directory
    files = os.listdir(directory_path)
    # Filter out directories (if any)
    text_files = [file for file in files if file.endswith(".txt")]
    return text_files


import os
import subprocess

def create_directory(directory_path):
    # Check if the directory already exists
    if os.path.exists(directory_path):
        print(f"Directory '{directory_path}' already exists. Skipping creation.")
        return

    try:
        # Create the directory at the specified path
        os.makedirs(directory_path)
        print(f"Directory '{directory_path}' created successfully.")
    except OSError as e:
        print(f"Error creating directory '{directory_path}': {e}")

# Example usage:
# new_directory_path = "/path/to/your/new_directory"
# create_directory(new_directory_path)

# Run a subprocess command (e.g., ls) without creating the directory again
# subprocess.run(["ls", new_directory_path])


def run_shell_command(command):
     # Run the shell command
     result = subprocess.run(command, check=True, text=True, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

def add_directoryNames(file_names_list,dir_path):
    new_list=[]
    for i in range(len(file_names_list)):
        new_list.append(dir_path+file_names_list[i])
    return new_list

def find_block_size(list_sample,a,b):
    """returns the size of list between a and b"""
    count=0
    for element in list_sample:
        if element&lt;=b and element&gt;=a:
            count+=1
    return count


def uncorrelate(input_file_accepted,measurement_index_accepted,output_file,autocorrelation_time):
    """this function supposed to take input as all measurement file_combined/ or nodewise which contains only accepted measurements, and return the file with mean of blocks(where blocksize is determined by autocorrelation_time)
The idea is to take all accepted measurements and rather than just take measurement at tau,2tau, 3tau....we make blocks of all accepted measurement and find mean of each blocks and save them with final data points which then can be used to find total estimate of observable and error estimation. Here block size is determined by autocorrelation_time, i.e. for the time interval of autocorrelation_time let us say we have 3 accepted measurements then we just make a data point with mean of the block and the time to be average of time instants of boundary of block.
for chi_ud the input_file_accepted = chi_ud_mean_list
for plaquette : the input_file_accepted = plaquette_accepted.txt
measurement index will help in finding the data points in a block
"""
    tau=autocorrelation_time
    with open(input_file_accepted, "r") as readfile:
        input_file_content_list=readfile.readlines()
    with open(measurement_index_accepted,"r") as readfile:
        measurement_index_content_list=readfile.readlines()
    mean_block=[]
    initial_element_list=[]
    final_element_list=[]
    block_list=[]
    for i in range(int(measurement_index_content_list[0]),int(measurement_index_content_list[-1])+1,tau):
        initial_element_list.append(i)
    for i in range(int(measurement_index_content_list[0])-1+tau,int(measurement_index_content_list[-1])+1,tau):
        final_element_list.append(i)
    if final_element_list[-1]!=int(measurement_index_content_list[-1]):
        final_element_list.append(int(measurement_index_content_list[-1]))
    for i in range(len(final_element_list)):
        block_list.append(find_block_size(measurement_index_content_list,initial_element_list[i],final_element_list[i]))
    # now block size list is ready

def remove_zeros(input_list):
    output_list=[]
    for value in input_list:
        if value!=0:
            output_list.append(value)
    return output_list



def uncorrelate_list(input_list_accepted,measurement_index_accepted,autocorrelation_time):
    """this function supposed to take input as all measurement file_combined/ or nodewise which contains only accepted measurements, and return the file with mean of blocks(where blocksize is determined by autocorrelation_time)
The idea is to take all accepted measurements and rather than just take measurement at tau,2tau, 3tau....we make blocks of all accepted measurement and find mean of each blocks and save them with final data points which then can be used to find total estimate of observable and error estimation. Here block size is determined by autocorrelation_time, i.e. for the time interval of autocorrelation_time let us say we have 3 accepted measurements then we just make a data point with mean of the block and the time to be average of time instants of boundary of block.
for chi_ud the input_file_accepted = chi_ud_mean_list
for plaquette : the input_file_accepted = plaquette_accepted.txt
measurement index will help in finding the data points in a block
"""
    tau=autocorrelation_time
    mean_block_list=[]
    initial_element_list=[]
    final_element_list=[]
    block_list=[]
    for i in range(int(measurement_index_accepted[0]),int(measurement_index_accepted[-1])+1,tau):
        initial_element_list.append(i)
    for i in range(int(measurement_index_accepted[0])+tau-1,int(measurement_index_accepted[-1])+1,tau):
        final_element_list.append(i)
    if final_element_list[-1]!=int(measurement_index_accepted[-1]):
        final_element_list.append(int(measurement_index_accepted[-1]))
    for i in range(len(final_element_list)):
        block_list.append(find_block_size(measurement_index_accepted,initial_element_list[i],final_element_list[i]))
    size=len(block_list)
    i=0
    while(i&lt;size):
        while (len(input_list_accepted[:block_list[i]])==0):
            final_element_list[i]=0 # we shall remove the time and data for which there are no measurements in a given block 
            i+=1
            if i==size:
                break
        if i&gt;=size:
            break
        mean_block_list.append(mean_list(input_list_accepted[:block_list[i]]))
        input_list_accepted=input_list_accepted[block_list[i]:]
        i+=1
    return block_list,remove_zeros(final_element_list),mean_block_list

def correlation_time(measurement_list):
    """this function accepts the measurement list (including declined ones.) and returns the autocorrelation_time.The autocorrelation_time time is calculated from time t=0 and step size=1, once calculated you can add the autocorrelation_time to measurement index"""
    import math
    # Access the value of e
    e_value = math.e
    chi_list=[]
    time=[]
    # print(measurement_list)
    for t in range(len(measurement_list)):
        # print(t,measurement_list[t])
        s=0.0
        s1=0
        s2=0
        factor=(1/(len(measurement_list)-t))
        for i in range(len(measurement_list)-t):
            s+=measurement_list[i]*measurement_list[i+t]
            s1+=measurement_list[i]
            s2+=measurement_list[i+t]
        chi_list.append(factor*(s-factor*s1*s2))
        time.append(t)

    #normalising autocorrelation_function
    chi_list=[element/chi_list[0] for element in chi_list]
    for i in range(len(chi_list)):
        if (chi_list[i]&lt;=(chi_list[0]/e_value)):
            return i,chi_list,time


def jackknife(measurement_list):
    """this function will take input as measurement list (which are accepted and are refined on intervals of autocorrelation_time) and returns the standard deviation calculated from jackknife method."""
    diff=0
    for i in range(len(measurement_list)):
        diff+=(mean_list(measurement_list[:i]+measurement_list[i+1:])-mean_list(measurement_list))**2
    return diff**.5


def randomise_list(sample_list):
    """this function will return the len(sample_list) numbers as list which are randomly choosen (repetition is allowed) """
    import random
    randomised_list=[]
    for i in range(len(sample_list)):
        rand_number=random.randint(0,len(sample_list))
        randomised_list.append(sample_list[rand_number])
    return randomised_list



def bootstrap(measurement_list):
    """this function returns bootstrap error"""
    mean_bootstrap=[]
    squared_mean=[]
    for i in range(1000):
        randomise_list_data=randomise_list(measurement_list)
        mean_bootstrap.append(mean_list(randomise_list_data))
        squared_mean.append(mean_list([element**2 for element in randomise_list_data]))

    bootstrap_deviation = (mean_list([element**2 for element in squared_mean]) - mean_list(mean_bootstrap)**2)**0.5
    return  bootstrap_deviation



def correct_measurement_index(input_list_measurement,number_of_all_measurements,number_of_nodes):
    """when we combine the outpout of individual nodes, the measurement index don't get updated, it repeats itself, so we shall add the length of measurement to next measurement index"""
    output_list=[]
    initial_element=int(input_list_measurement[0])
    for j in range(number_of_nodes):
        for i in range(number_of_all_measurements):
            output_list.append(j*int(number_of_all_measurements)+(int(input_list_measurement[i])))
    return output_list

def individual_node_plots(dir_path,input_file_list,generated_files_dir):
    """this function does all the analysis for finding mean, generating plot per node for chi_ud, chi_s and plaquette observable , the first parameter is directory path of all node file outputs of rhmc, second parameter is list of those all .txt files of individual nodes, the third parameter is string (a name for all new plots and files to be saved), this function does not make use of autocorrelation_function , this function assumes uncorrelation between all data points"""
    accepted_file_list=[]
    acceptance_ratio_list=[]
    advanced_filtering_list_ud=[]
    advanced_filtering_list_s=[]
    plaquette_node_filelist=[]
    chi_ud_mean_list=[]
    chi_s_mean_list=[]
    measurement_index_accepted=[]
    input_file_list_new=add_directoryNames(input_file_list,dir_path)
    for i in range(len(input_file_list_new)):
        outputfilename=(dir_path+generated_files_dir+'/'+input_file_list[i])[:-4]+"_accepted.txt"
        accepted_filtering(input_file_list_new[i],outputfilename,"Update accepted")
        accepted_file_list.append(outputfilename) # creating file for each _accepted MEASUREMENT
        acceptance_ratio_list.append(acceptance_ratio(input_file_list_new[i]))
# once the accepted files are generated we can use advanced_filtering to make files with chi_ud, chi_s, Plaquette

    for accepted_file in accepted_file_list:
        advanced_filtering(accepted_file,accepted_file[:-4]+"_chi_ud.txt","CHI_UD = ")
        advanced_filtering(accepted_file,accepted_file[:-4]+"_chi_s.txt","CHI_S = ")
        advanced_filtering(accepted_file,accepted_file[:-4]+"_plaquette.txt","Plaquette = ")
        advanced_filtering(accepted_file,accepted_file[:-4]+"_index.txt","MEASUREMENT: ")
# writing the list
        advanced_filtering_list_ud.append(accepted_file[:-4]+"_chi_ud.txt")
        advanced_filtering_list_s.append(accepted_file[:-4]+"_chi_s.txt")
        plaquette_node_filelist.append(accepted_file[:-4]+"_plaquette.txt")
        measurement_index_accepted.append(accepted_file[:-4]+"_index.txt")
# once the advanced_filtering has been done we will create the files for mean of light and strange quarks
    for element in advanced_filtering_list_s:
        chiral_condensate_mean(element,element[:-4]+"_mean.txt")
        chi_s_mean_list.append(element[:-4]+"_mean.txt")
    for element in advanced_filtering_list_ud:
        chiral_condensate_mean(element,element[:-4]+"_mean.txt")
        chi_ud_mean_list.append(element[:-4]+"_mean.txt")
# Now the mean files have been generated, I can now proceed to generate plots and save them
    with open(dir_path+generated_files_dir+"/"+"THE_RESULT.txt","a") as writefile:
        writefile.write("\n\n\n\nfile_name\t\tMean_chi_ud\t\tStandard Deviation\t\tAcceptance Ratio\n")
        for i in range(0,len(input_file_list)):
            plot_fun(measurement_index_accepted[i],chi_ud_mean_list[i],'r')
            writefile.write((input_file_list[i])+"\t\t"+str(round((mean_file(chi_ud_mean_list[i])),7))+"\t\t"+str(round((standard_deviation_file(chi_ud_mean_list[i])),7))+"\t\t"+str(acceptance_ratio_list[i])+"\n")
        writefile.write("\n\nfile_name\t\tMean_chi_s\t\tStandard Deviation\t\tAcceptance Ratio\n")
        for i in range(0,len(input_file_list)):
            plot_fun(measurement_index_accepted[i],chi_s_mean_list[i],'g')
            writefile.write((input_file_list[i])+"\t\t"+str(round((mean_file(chi_s_mean_list[i])),7))+"\t\t"+str(round((standard_deviation_file(chi_s_mean_list[i])),7))+"\t\t"+str(acceptance_ratio_list[i])+"\n")
        writefile.write("\n\nfile_name\t\tMean_plaquette\t\tStandard Deviation\t\tAcceptance Ratio\n")
        for i in range(0,len(input_file_list)):
            writefile.write((input_file_list[i])+"\t\t"+str(round((mean_file(plaquette_node_filelist[i])),7))+"\t\t"+str(round((standard_deviation_file(plaquette_node_filelist[i])),7))+"\t\t"+str(acceptance_ratio_list[i])+"\n")
            plot_fun(measurement_index_accepted[i],plaquette_node_filelist[i],'o')



def calculation(dir_path,file_names_list,generated_files_dir):
    """this function will handle all things in terms of lists and calculates expactation value of observables of rhmc(which are chi_ud, chi_s, plaquette), this function will also save the respective plots of quantity with respect to monte carlo time, we will find correlation_time and find mean and standard deviation of the observable"""
    # let us first access the file names list which are added with directory path
    file_names=add_directoryNames(file_names_list,dir_path)
    mean_master_list_chi_ud=[]
    mean_master_list_chi_s=[]
    mean_master_list_plaquette=[]
    mean_master_list_delta_h=[]
    std_deviation_master_list_chi_ud=[]
    std_deviation_master_list_chi_s=[]
    std_deviation_master_list_plaquette=[]
    std_deviation_master_list_delta_h=[]
    jackknife_master_list_chi_ud=[]
    jackknife_master_list_chi_s=[]
    jackknife_master_list_plaquette=[]
    jackknife_master_list_delta_h=[]
    chi_ud_corr_time_master=[]
    chi_s_corr_time_master=[]
    delta_h_corr_time_master=[]
    plaquette_corr_time_master=[]
    # since we will be only storing the plots in generated files directory, let us leave that for a moment
    # we shall calculate the correlation time for each node(gpu) file
    # first we shall calculate correlation time for plaquette, we have to include all measurements including declined ones. 
    for index in range(len(file_names)):
        with open(file_names[index],"r") as readfile:
            # let us define the sublist for all perticular files.We shall append these nodewise sublists to master list for each calculation.
            initial_content=readfile.readlines()
            delta_h_value_all=advanced_filtering_list(initial_content,"Delta H = ")
            plaquette_value_all=advanced_filtering_list(initial_content,"Plaquette = ")
            # print(plaquette_value_all)
            plaquette_corr_time,corr_fun_plaquette,time_plaquette=correlation_time(plaquette_value_all)
            delta_h_corr_time,corr_fun_delta_h,time_delta_h=correlation_time(delta_h_value_all)
            # to calculate correlation time using chi_ud and chi_s, we shall first store the mean of all accepted and rejected monte carlo measurements 
            chi_ud_all=advanced_filtering_list(initial_content,"CHI_UD = ")
            chi_s_all=advanced_filtering_list(initial_content,"CHI_S = ")
            chi_ud_all_mean_ten=chiral_condensate_mean_list(chi_ud_all)
            chi_s_all_mean_ten=chiral_condensate_mean_list(chi_s_all)
            # now we have the data for mean for all measurements including declined ones, we shall invoke the function to calculate the correlation time. 
            chi_ud_corr_time,corr_fun_ud,time_ud=correlation_time(chi_ud_all_mean_ten)
            chi_s_corr_time,corr_fun_s,time_s=correlation_time(chi_s_all_mean_ten)
            # now we have the correlation time calculated, we shall use that time to take independent measurements. Let me choose the chi_ud_corr_time as correlation time. we shall now filter the accepted measurements in that time interval. Rather than just ignoring these data points which lies between tau and 2 tau, we shall take the mean of all data points which lie in time interval of tau.  
            tau=chi_s_corr_time
            # let us save the plots of autocorrelation function
            save_fig_path_dir=(dir_path+generated_files_dir+"/"+file_names_list[index])
            lim_list=[0,50]
            plot_fun_list(time_plaquette,corr_fun_plaquette,'b',save_fig_path_dir+"autocorrelation_function_plaquette.png","Plaquette",lim_list,plaquette_corr_time)
            plot_fun_list(time_delta_h,corr_fun_delta_h,'m',save_fig_path_dir+"autocorrelation_function_delta_h.png","delta_h",lim_list,delta_h_corr_time)
            plot_fun_list(time_ud,corr_fun_ud,'r',save_fig_path_dir+"autocorrelation_function_chi_ud.png","CHI_UD",lim_list,chi_ud_corr_time)
            plot_fun_list(time_s,corr_fun_s,'g',save_fig_path_dir+"autocorrelation_function_chi_s.png","Plaquette",lim_list,chi_s_corr_time)
            chi_ud_corr_time_master.append(chi_ud_corr_time)
            chi_s_corr_time_master.append(chi_s_corr_time)
            delta_h_corr_time_master.append(delta_h_corr_time)
            plaquette_corr_time_master.append(plaquette_corr_time)

            # we shall now use the accepted measurements mean list and generate the list 
            accepted_list=accepted_filtering_list(initial_content,"Update accepted")
            #once we have accepted data, we can do advanced filtering and find mean list of 10.
            # accepted_list=initial_content# checking if we do the calculation with all data
            chi_ud_accepted_list=advanced_filtering_list(accepted_list,"CHI_UD = ")
            chi_s_accepted_list=advanced_filtering_list(accepted_list,"CHI_S = ")
            measurement_index_accepted=advanced_filtering_list(accepted_list,"MEASUREMENT: ")
            if file_names_list[index]=="combined.txt":
                measurement_index_accepted=correct_measurement_index(measurement_index_accepted,int(len(measurement_index_accepted)/(len(file_names_list)-1)),len(file_names_list)-1)
                # we had to substract 1 as it also contains the combined.txt file , and we just want number of nodes
            #finding mean list of 10
            chi_ud_accepted_mean_list=chiral_condensate_mean_list(chi_ud_accepted_list)
            chi_s_accepted_mean_list=chiral_condensate_mean_list(chi_s_accepted_list)
            plaquette_accepted_list=advanced_filtering_list(accepted_list,"Plaquette = ")
            delta_h_accepted_list=advanced_filtering_list(accepted_list,"Delta H = ")
            #now we have all the accepted data, we just have to find mean of all data inside blocks of interval tau
            # so we need a function which takes a list, tau, and returns list with sublist with blocksize.
            # we have such function named uncorrelate_list which will take input_list_accepted, measurement_index_accepted and correlation_time and return the time index list and mean block list
            block_size_chi_ud_accepted,time_list_chi_ud_accepted,chi_ud_block_mean=uncorrelate_list(chi_ud_accepted_mean_list,measurement_index_accepted,tau)
            block_size_chi_s_accepted,time_list_chi_s_accepted,chi_s_block_mean=uncorrelate_list(chi_s_accepted_mean_list,measurement_index_accepted,tau)
            block_size_plaquette_accepted,time_list_plaquette_accepted,plaquette_block_mean=uncorrelate_list(plaquette_accepted_list,measurement_index_accepted,tau)
            block_size_delta_h_accepted,time_list_delta_h_accepted,delta_h_block_mean=uncorrelate_list(delta_h_accepted_list,measurement_index_accepted,tau)
            if (len(chi_s_block_mean)!=len(time_list_chi_s_accepted)) and (len(chi_ud_block_mean)!=len(time_list_chi_ud_accepted)) and (len(plaquette_block_mean)!=len(time_list_plaquette_accepted)):
                print("something went wrong in finding uncorrelate_list function")
            plot_fun_list(time_list_chi_ud_accepted,chi_ud_block_mean,'r',save_fig_path_dir+"_CHI_UD.png","CHI_UD")
            plot_fun_list(time_list_chi_s_accepted,chi_s_block_mean,'g',save_fig_path_dir+"_CHI_S.png","CHI_S")
            plot_fun_list(time_list_plaquette_accepted,plaquette_block_mean,'b',save_fig_path_dir+"_plaquette.png","Plaquette")
            plot_fun_list(time_list_delta_h_accepted,delta_h_block_mean,'m',save_fig_path_dir+"_delta_h.png","delta_h")

            # Plots have been saved, let us write the mean, standard deviation and write it to some result_list file
            mean_master_list_chi_ud.append(mean_list(chi_ud_block_mean))
            mean_master_list_chi_s.append(mean_list(chi_s_block_mean))
            mean_master_list_plaquette.append(mean_list(plaquette_block_mean))
            mean_master_list_delta_h.append(mean_list(delta_h_block_mean))
            std_deviation_master_list_chi_ud.append(standard_deviation_list(chi_ud_block_mean))
            std_deviation_master_list_chi_s.append(standard_deviation_list(chi_s_block_mean))
            std_deviation_master_list_plaquette.append(standard_deviation_list(plaquette_block_mean))
            std_deviation_master_list_delta_h.append(standard_deviation_list(delta_h_block_mean))
            jackknife_master_list_chi_ud.append(jackknife(chi_ud_block_mean))
            jackknife_master_list_chi_s.append(jackknife(chi_s_block_mean))
            jackknife_master_list_plaquette.append(jackknife(plaquette_block_mean))
            jackknife_master_list_delta_h.append(jackknife(delta_h_block_mean))

    # we shall write the results in a single file
    results=dir_path+generated_files_dir+"/"+"THE_RESULT_list_all.txt"
    with open(results,'w') as writefile:
        writefile.write("File names\t\tMean_CHI_UD\t\tStd deviation\t\tJackknife\t\tcorrelation time\n")
        for i in range(len(mean_master_list_chi_ud)):
            writefile.write(file_names_list[i]+"\t\t"+str(round((mean_master_list_chi_ud[i]),6))+"\t\t"+str(round((std_deviation_master_list_chi_ud[i]),6))+"\t\t  "+str(round((jackknife_master_list_chi_ud[i]),6))+"\t\t"+str(chi_ud_corr_time_master[i])+"\n")
        writefile.write("\n\n\n")
        writefile.write("File names\t\tMean_CHI_S\t\tStd deviation\t\tJackknife\t\tcorrelation time\n")
        for i in range(len(mean_master_list_chi_s)):
            writefile.write(file_names_list[i]+"\t\t"+str(round((mean_master_list_chi_s[i]),6))+"\t\t"+str(round((std_deviation_master_list_chi_s[i]),6))+"\t\t  "+str(round((jackknife_master_list_chi_s[i]),6))+"\t\t"+str(chi_s_corr_time_master[i])+"\n")
        writefile.write("\n\n\n")
        writefile.write("File names \t\tMean_Plaquett\t\tStd deviation\t\tJackknife\t\tcorrelation time\n")
        for i in range(len(mean_master_list_plaquette)):
            writefile.write(file_names_list[i]+"\t\t"+str(round((mean_master_list_plaquette[i]),6))+"\t\t"+str(round((std_deviation_master_list_plaquette[i]),6))+"\t\t  "+str(round((jackknife_master_list_plaquette[i]),6))+"\t\t"+str(plaquette_corr_time_master[i])+"\n")
        writefile.write("\n\n\n")
        writefile.write("File names \t\tMean_Delta_H\t\tStd deviation\t\tJackknife\t\tcorrelation time\n")
        for i in range(len(mean_master_list_delta_h)):
            writefile.write(file_names_list[i]+"\t\t"+str(round((mean_master_list_delta_h[i]),6))+"\t\t"+str(round((std_deviation_master_list_delta_h[i]),6))+"\t\t  "+str(round((jackknife_master_list_delta_h[i]),6))+"\t\t"+str(delta_h_corr_time_master[i])+"\n")
# let us just print the content to terminal
    with open(results,"r") as readfile:
        for line in readfile:
            print(line,end='')
</pre>

    </main>

    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js"></script>
    <script type="text/javascript">
        document.querySelectorAll('pre').forEach(block => hljs.highlightBlock(block));
    </script>
</body>
</html>

